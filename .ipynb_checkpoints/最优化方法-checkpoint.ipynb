{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f115b3d9-5b1c-40c2-97e0-9b1fafab8e25",
   "metadata": {},
   "source": [
    "# 最优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215b770b-89c9-4f08-b0b1-e53a3aca36d6",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b769166-fb99-4253-b634-ef6a090901e3",
   "metadata": {},
   "source": [
    "### 问题定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21966725-5e1e-44d5-a8a2-616e96e8f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标函数 优化变量\n",
    "# 可行域: 由目标函数的定义域, 等式, 以及不等式约束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74bad976-e984-42c7-b744-15f389a56446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果目标函数为一次函数, 则称为线性规划\n",
    "# 如果不是非线性函数, 则称为非线性规划. 一种特例是二次函数, 称为二次规划"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711bd8aa-c755-4fb1-8a30-b963a3e44ceb",
   "metadata": {},
   "source": [
    "### 迭代法的思想"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ff4edf2-96bd-413f-8f4f-e34379e3d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求解梯度为0的点往往是非常复杂的事情, 所以需要一些比较好的方法\n",
    "# 没有公式解的方程称为超越方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383a7c9-7d01-4a6d-a2a6-16bb139f7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数值优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed4855-8c64-4590-9726-29296ae318c4",
   "metadata": {},
   "source": [
    "## 一阶优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8376d5a-d394-42eb-9455-7642d90ec35b",
   "metadata": {},
   "source": [
    "### 梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfd532f-c490-4c43-ae3b-e87c2a40fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公式: Xk+1 = Xk + deltaX\n",
    "    ##  Xk+1 = Xk - 梯度*学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3375d649-5855-4def-a4a4-1c8769f34d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停止的条件: 达到固定的迭代次; 梯度值已经很小了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30339946-74f8-40be-bd6a-9e93a1574a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 步长的设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8026ec-89f9-415b-9168-ca108bd02fef",
   "metadata": {},
   "source": [
    "### 最速下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39956e06-4236-45f5-8963-02bad70a726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对梯度下降法的改进, 能够每次计算最佳步长值\n",
    "# 计算方法: 一种是离散化尝试, 一种是最优化这个问题, 线性搜索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df668f6e-64d1-4f60-93a7-4642ecbde66b",
   "metadata": {},
   "source": [
    "### 梯度下降法的改进"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa2860f-ea5d-4b4e-8956-4d5001855ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态项梯度下降法: 使用了前k次的负梯度值对当前梯度值进行修正, 并使用了移动指数加权平均\n",
    "# AdaGrad算法: 利用历史的梯度值对学习率自适应计算, 以往的梯度值越大, 学习率越小\n",
    "# RMSProp算法: 对AdaGrad的改进, 避免长期累计梯度值导致学习率趋于0的问题, \n",
    "            ## 使用移动指数加权平均对梯度进行衰减\n",
    "# AdaDelta算法: 使用移动指数加权平均对梯度进行衰减; 去掉人工设置的全局学习率的依赖\n",
    "# Adam算法: 使用了自适应学习率和动量项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df5aa2-f815-4423-83f1-661b8f678ff1",
   "metadata": {},
   "source": [
    "### 随机梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60212292-cfde-470a-ba4c-ec39088d25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有样本求损失函数梯度比较复杂, 可以每次迭代时选取一批样本, 近似使用这些样本代替整个样本\n",
    "\n",
    "# 小批量随机梯度下降法\n",
    "# 具体操作: 每次先对所有训练样本进行随机洗牌, 然后均匀分成多份, 每份M个样本, 接下来依次用\n",
    "            # 每一份执行梯度下降法迭代."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333d2de-99bc-4e8f-80ff-b1defadc42f4",
   "metadata": {},
   "source": [
    "### 应用: 人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f30be3-0611-4e77-a85f-7acd484ff09b",
   "metadata": {},
   "source": [
    "## 二阶优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172273cd-fdc7-4146-bbb1-63a75d7624c1",
   "metadata": {},
   "source": [
    "### 牛顿法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e0ef628-a84d-406d-a18d-b1567ac71578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xk+1 = xk - a Hk-1 gk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48177d5-ae9b-4e69-901f-db2afc7fbdbd",
   "metadata": {},
   "source": [
    "### 拟牛顿法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0dc0fed-bdb4-4769-a9ef-ad7335e732c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DFP: 在求黑塞矩阵的逆比较困难, 所以利用矩阵近似\n",
    "# BFGS: 黑塞矩阵的近似"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13be03-e6af-4bd8-8c1c-28e3feb8e638",
   "metadata": {},
   "source": [
    "## 分治法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5981701-ca11-4860-8be3-c1103b8fec78",
   "metadata": {},
   "source": [
    "### 坐标下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed57e42-63ee-47b0-9ad7-72c40a9236ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于不可导点不能用这个方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae805f-d8f1-4628-a5f0-81f93826c86e",
   "metadata": {},
   "source": [
    "### SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0e36d3-f9b3-4342-8da8-fdd85a566f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每次从优化变量中挑出两个变量进行优化, 保证满足等式约束条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094383b-a98b-4717-8ed6-4aa82419d1a8",
   "metadata": {},
   "source": [
    "### 分阶段优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65dac860-835f-4f6a-b30c-cb4138a8aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost算法是对beta和f的双阶段优化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b0a75-2977-4d6d-892e-0631ab9f4e8b",
   "metadata": {},
   "source": [
    "### 应用: logistics回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e91bd-4849-46dd-95e5-9455d250612f",
   "metadata": {},
   "source": [
    "## 凸优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfd2a2-85a8-4956-918f-911fc7cb5246",
   "metadata": {},
   "source": [
    "### 数值优化算法面临的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2d7a02-5f65-4175-9336-cf18cbec135f",
   "metadata": {},
   "source": [
    "### 凸集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9a06f-31e7-4328-bc7a-ef1efea34836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一组线性等式约束条件定义的可行域是凸集\n",
    "# 多面体也是凸集, 他是由线性不等式组定义的集合\n",
    "# 下水平集(sub level set): 凸函数的下水平集还是凸集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624a720-4073-4080-953e-87d9a9338c9b",
   "metadata": {},
   "source": [
    "### 凸优化问题及其性质"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f0531b-ab95-4da8-8b05-3f2affe16eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于凸优化问题, 所有局部最优解都是全局最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965503d-0036-4cad-9742-8897c275e508",
   "metadata": {},
   "source": [
    "### 机器学习中的凸优化问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82458af-f0d7-4fe6-b25f-73b89ad07a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性回归\n",
    "# logistics回归\n",
    "# 支持向量机\n",
    "# softmax回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8908fb-f4ee-4347-973f-b7328318234e",
   "metadata": {},
   "source": [
    "## 等约束的优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ed8e3-5464-4b1b-adca-b63d90744593",
   "metadata": {},
   "source": [
    "### 拉格朗日乘数法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "874ed4b7-4a89-49ca-8321-4c9885274dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉格朗日乘数法是用于求解带等式约束条件的函数极值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbabf245-f529-448f-9c12-d2945ea52b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉格朗日乘数法的几何解释: 在极值点处目标函数的梯度是约束函数梯度的线性组合\n",
    "# 这个就是港大的教授所讲的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27785a-edbe-47ad-8219-5e83fd7b9560",
   "metadata": {},
   "source": [
    "### 应用: 线性判别分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735bd50e-27a6-4844-b89f-20b884343923",
   "metadata": {},
   "source": [
    "### 拉格朗日对偶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1509ec-78b4-4be0-b3de-fa37894b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉格朗日函数\n",
    "# 优化问题改写: max拉格朗日函数\n",
    "# 弱对偶定理\n",
    "# 对偶间隙\n",
    "# 当对偶间隙为0时, 我们可以把求解原问题转化为对偶问题, 取到等号时为强对偶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4735dd1e-9242-40ec-872b-345e2e0b41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 强对偶成立的充分条件: slater条件\n",
    "# slater条件: 凸优化问题中, 至少有个可行点在不等式约束区域的内部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f4fe61-8d25-4928-987e-7a6fca71f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用拉格朗日对偶: 1.凸优化问题  2.满足slater条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00ffc1-2f02-4b7b-a260-5a7d18c3f062",
   "metadata": {},
   "source": [
    "### KKT条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0347e0c-3650-4391-8694-61a4e279f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求解带有不等式和等式约束的优化问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa0c3a7-c718-4771-8176-4844ef051039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KKT条件是取得极值的必要条件而非充分条件, 但一般都是充要条件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee543a6-9d50-4771-9bbe-3496bdf501bc",
   "metadata": {},
   "source": [
    "### 应用: 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589cc89-a0a3-4f64-8ad5-2b13947633a2",
   "metadata": {},
   "source": [
    "## 多目标优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78760cfb-4942-40ba-8625-712235c4e791",
   "metadata": {},
   "source": [
    "### 基本概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a75c5e2a-de7c-4456-901f-041ae182c75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 帕累托改进\n",
    "# 帕累托最优解\n",
    "# 帕累托最优解可能是一个集合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed529832-fe80-4aad-a445-2db33374d869",
   "metadata": {},
   "source": [
    "### 求解算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8209452b-d593-4a43-a1c3-347765a3e745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标量化: 线性标量化\n",
    "# 一匹龙约束法: 通过将其他目标函数转化为不等式约束"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02d92c-1220-473a-a88c-afb79d06f20f",
   "metadata": {},
   "source": [
    "### 应用: 多目标神经结构搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b8ea0c-c9b6-410f-a191-cf740863e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经结构搜索: 自动化机器学习\n",
    "# 多目标神经结构搜索: \n",
    "# MONAS: 线性标量法的应用\n",
    "# MnasNet: 加权乘积法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad2f97-ee94-4a98-8482-205082d245ef",
   "metadata": {},
   "source": [
    "## 泛函极值与变分法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82666a1-5156-47ef-b1ee-22fb3b7a9392",
   "metadata": {},
   "source": [
    "### 泛函与变分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da104c76-5268-41ed-b1f5-5e6a071fa9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化的变量不是数, 而是函数\n",
    "# 变分法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a961f9a0-f12d-4ede-94d2-c8dca4424827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数空间\n",
    "# 泛函一般是一个函数集合的定积分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56a50bea-94ba-47a9-9fe0-4a63444ca5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 泛函的极值: 对于一个给定的函数f, 如果其他函数都大于或者小于他"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e26b52e-f12d-4d2d-8bb3-f2130ab517ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数的距离: 在定义域内的, 所有函数值或者导数值的差值的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b72a47-c0b3-4035-bdec-ee39667565c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变分: 如果两个函数f在a和b点处的函数值相等, 那么y(x)在y2(x)点处的变分为y(x)-y2(x)\n",
    "# 泛函的核"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cf4000-ca9d-488a-8e4d-67f787a5a252",
   "metadata": {},
   "source": [
    "### 欧拉-拉格朗日方程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12981e7a-e175-498c-8684-0b5327b12d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 他是变分法的核心\n",
    "# 通过求解变分为0的点而求解泛函的极值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "815108fa-b88e-4745-9207-0622173653cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 泛函取得极值的必要条件: 欧拉-拉格朗日方程 \n",
    "# L对y的偏导 - L对y'的偏导再对x偏导 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51718e1f-2ac1-435a-90c3-6d4c8bae4e0e",
   "metadata": {},
   "source": [
    "## 目标函数的构造"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4920c42-a471-4c66-a331-6b6ff9192a2d",
   "metadata": {},
   "source": [
    "### 有监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a5570c-a11b-4bbf-926f-6f93101cddc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 感知器损失\n",
    "# 合页损失(Hinge Loss)函数\n",
    "# 指数损失函数: Adaboost使用\n",
    "# 欧氏距离损失函数: 二分类, 多分类(one-hot)\n",
    "# 交叉熵损失函数: 用在多分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b443ec9-691c-4c0c-869f-0adfff8d54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归问题: 欧氏距离, 绝对值损失, huber损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa25a6a-97f0-43d9-8cd9-42fa74d08c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多目标损失函数: 多用于图像检测, 第一部分为分类损失, 第二部分为定位损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42d345-0ead-429d-9793-422a736dea7b",
   "metadata": {},
   "source": [
    "### 无监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8cf762-1e79-4f4f-bd6f-b99c5635cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类\n",
    "# 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c69ac-449e-4a32-9a31-1875e3d620d9",
   "metadata": {},
   "source": [
    "### 强化学习"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
